\section{Theoretical validation}
\begin{frame}{\secname}
	\framesubtitle{System identification basic}
	\begin{columns}
		\begin{column}{0.5\textwidth}
			\begin{itemize}
				\item Assume that a data set $Z^N = \{u[n],y[n]|n=1\ldots N\}$ has been collected.
				\item The dataset $Z^N$ is assumed generated by
					\begin{equation}
						\mathcal{S}: y[n] = G_0(z,\theta_0)u[n] + H_0(z,\theta_0)e[n]
					\end{equation}
				\item Using the data set $Z^N$ we want to find the parameter vector $\theta^N$ minimizing
\begin{equation}\label{eq:pred}
		\hat{\theta}_N = \argmin_{\theta} \frac{1}{N}\sum_{n=1}^N \epsilon^2(n,\theta)
\end{equation}
			\end{itemize}
		\end{column}
		\begin{column}{0.5\textwidth}
			\begin{figure}
				\includegraphics{./pictures/v_block.tikz}
			\end{figure}
		\end{column}
	\end{columns}
\end{frame}
\begin{frame}{\secname}
	\framesubtitle{Consistency}
	\begin{itemize}
	\item A consistent estimate means that the true parameter vector $\theta_0$ is the unique solution to the asymptotic prediction error criterion.
\begin{equation}\label{eq:theta_1}
		\theta^*= \argmin_{\theta} \bar{E}\epsilon^2(n,\theta)
\end{equation}
with
\begin{equation}
		\bar{E}\epsilon^2(n,\theta) = \lim_{N\to\infty}\frac{1}{N}\sum_{t=1}^N E\epsilon^2(n,\theta)
\end{equation}
and
\begin{equation}\label{eq:epsi}
		\epsilon(n,\theta)=H_1^{-1}(z,\theta)(y[n]-G_1(z,\theta)u[n])
\end{equation}
\end{itemize}
\end{frame}
